"""
Depth Anything V2
æ›´ç²¾ç¡®çš„æ·±åº¦ä¼°è®¡å’Œåº¦é‡æ·±åº¦æ”¯æŒ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional, Tuple
import cv2


class DepthAnythingV2:
    """Depth Anything V2 - å‡çº§ç‰ˆ"""
    
    def __init__(
        self,
        model_size: str = "vitl",  # vits/vitb/vitl/vitg
        metric_depth: bool = True,  # å¯ç”¨åº¦é‡æ·±åº¦
        device: str = "cuda",
        max_depth: float = 20.0  # æœ€å¤§æ·±åº¦(ç±³)
    ):
        self.device = device
        self.model_size = model_size
        self.metric_depth = metric_depth
        self.max_depth = max_depth
        
        print(f"ğŸ”„ Loading Depth Anything V2: {model_size}")
        
        try:
            from depth_anything_v2.dpt import DepthAnythingV2
            
            # æ¨¡å‹é…ç½®
            model_configs = {
                'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
                'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
                'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
                'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
            }
            
            # åŠ è½½æ¨¡å‹
            model_config = model_configs[model_size]
            self.model = DepthAnythingV2(**model_config)
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            checkpoint_path = f"data/checkpoints/depth_anything_v2_{model_size}.pth"
            if metric_depth:
                checkpoint_path = checkpoint_path.replace('.pth', '_metric.pth')
            
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            self.model.load_state_dict(state_dict)
            self.model.to(device)
            self.model.eval()
            
            print(f"âœ“ Depth Anything V2 loaded: {model_size}")
            print(f"  Metric depth: {metric_depth}")
            print(f"  Max depth: {max_depth}m")
            
        except Exception as e:
            print(f"âŒ Failed to load Depth Anything V2: {e}")
            print("  Please install: pip install git+https://github.com/DepthAnything/Depth-Anything-V2.git")
            self.model = None
    
    @torch.no_grad()
    def estimate(
        self,
        image: np.ndarray,
        target_size: Optional[Tuple[int, int]] = None
    ) -> np.ndarray:
        """
        ä¼°è®¡æ·±åº¦ - V2å¢å¼ºç‰ˆ
        
        Args:
            image: RGBå›¾åƒ (H, W, 3), èŒƒå›´[0, 255]
            target_size: ç›®æ ‡å°ºå¯¸ (H, W)
            
        Returns:
            depth: æ·±åº¦å›¾ (H, W), å•ä½ç±³
        """
        if self.model is None:
            # Dummy depth
            H, W = image.shape[:2]
            return np.random.rand(H, W) * 5 + 2
        
        H, W = image.shape[:2]
        
        # é¢„å¤„ç†
        image_tensor = self._preprocess(image)
        image_tensor = image_tensor.to(self.device)
        
        # æ¨ç†
        depth = self.model(image_tensor)
        
        # åå¤„ç†
        depth = depth.squeeze().cpu().numpy()
        
        # è°ƒæ•´å°ºå¯¸
        if target_size is None:
            target_size = (H, W)
        
        if depth.shape != target_size:
            depth = cv2.resize(
                depth,
                (target_size[1], target_size[0]),
                interpolation=cv2.INTER_LINEAR
            )
        
        # åº¦é‡æ·±åº¦å¤„ç†
        if self.metric_depth:
            # V2ç›´æ¥è¾“å‡ºåº¦é‡æ·±åº¦ï¼Œé™åˆ¶èŒƒå›´
            depth = np.clip(depth, 0.1, self.max_depth)
        else:
            # ç›¸å¯¹æ·±åº¦è½¬åº¦é‡æ·±åº¦
            depth = self._normalize_depth(depth)
        
        return depth
    
    def estimate_with_confidence(
        self,
        image: np.ndarray,
        target_size: Optional[Tuple[int, int]] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä¼°è®¡æ·±åº¦å’Œç½®ä¿¡åº¦ - NEW
        
        Args:
            image: RGBå›¾åƒ
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            depth: æ·±åº¦å›¾ (H, W)
            confidence: ç½®ä¿¡åº¦å›¾ (H, W)
        """
        if self.model is None:
            H, W = image.shape[:2]
            depth = np.random.rand(H, W) * 5 + 2
            confidence = np.ones((H, W)) * 0.8
            return depth, confidence
        
        H, W = image.shape[:2]
        
        # é¢„å¤„ç†
        image_tensor = self._preprocess(image)
        image_tensor = image_tensor.to(self.device)
        
        # æ¨ç†ï¼ˆè·å–ä¸­é—´ç‰¹å¾ç”¨äºç½®ä¿¡åº¦ä¼°è®¡ï¼‰
        with torch.no_grad():
            depth = self.model(image_tensor)
            
            # ä¼°è®¡ç½®ä¿¡åº¦ï¼ˆåŸºäºæ·±åº¦æ¢¯åº¦çš„å¹³æ»‘åº¦ï¼‰
            depth_grad_x = torch.abs(depth[:, :, :, 1:] - depth[:, :, :, :-1])
            depth_grad_y = torch.abs(depth[:, :, 1:, :] - depth[:, :, :-1, :])
            
            # è¾¹ç•Œå¡«å……
            depth_grad_x = torch.nn.functional.pad(depth_grad_x, (0, 1, 0, 0))
            depth_grad_y = torch.nn.functional.pad(depth_grad_y, (0, 0, 0, 1))
            
            # æ¢¯åº¦è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            gradient_magnitude = torch.sqrt(depth_grad_x**2 + depth_grad_y**2)
            confidence = torch.exp(-gradient_magnitude * 10)  # æŒ‡æ•°è¡°å‡
            confidence = confidence.squeeze().cpu().numpy()
        
        depth = depth.squeeze().cpu().numpy()
        
        # è°ƒæ•´å°ºå¯¸
        if target_size is None:
            target_size = (H, W)
        
        if depth.shape != target_size:
            depth = cv2.resize(depth, (target_size[1], target_size[0]), 
                             interpolation=cv2.INTER_LINEAR)
            confidence = cv2.resize(confidence, (target_size[1], target_size[0]), 
                                  interpolation=cv2.INTER_LINEAR)
        
        # åº¦é‡æ·±åº¦å¤„ç†
        if self.metric_depth:
            depth = np.clip(depth, 0.1, self.max_depth)
        else:
            depth = self._normalize_depth(depth)
        
        return depth, confidence
    
    def estimate_multi_scale(
        self,
        image: np.ndarray,
        scales: list = [1.0, 0.75, 0.5],
        fusion_method: str = "weighted"
    ) -> np.ndarray:
        """
        å¤šå°ºåº¦æ·±åº¦ä¼°è®¡ - NEW
        
        Args:
            image: RGBå›¾åƒ
            scales: å°ºåº¦åˆ—è¡¨
            fusion_method: èåˆæ–¹æ³• ['mean', 'weighted', 'median']
            
        Returns:
            depth: èåˆåçš„æ·±åº¦å›¾
        """
        H, W = image.shape[:2]
        depths = []
        confidences = []
        
        for scale in scales:
            h, w = int(H * scale), int(W * scale)
            resized = cv2.resize(image, (w, h))
            
            depth, conf = self.estimate_with_confidence(resized, target_size=(H, W))
            depths.append(depth)
            confidences.append(conf)
        
        depths = np.stack(depths, axis=0)  # (N, H, W)
        confidences = np.stack(confidences, axis=0)  # (N, H, W)
        
        # èåˆ
        if fusion_method == "mean":
            fused_depth = depths.mean(axis=0)
        elif fusion_method == "weighted":
            weights = confidences / (confidences.sum(axis=0, keepdims=True) + 1e-8)
            fused_depth = (depths * weights).sum(axis=0)
        elif fusion_method == "median":
            fused_depth = np.median(depths, axis=0)
        else:
            fused_depth = depths[0]
        
        return fused_depth
    
    def compute_depth_edges(
        self,
        depth: np.ndarray,
        threshold: float = 0.1
    ) -> np.ndarray:
        """
        è®¡ç®—æ·±åº¦è¾¹ç¼˜ - NEW
        
        Args:
            depth: æ·±åº¦å›¾
            threshold: è¾¹ç¼˜é˜ˆå€¼
            
        Returns:
            edges: è¾¹ç¼˜å›¾ (H, W) bool
        """
        # Sobelè¾¹ç¼˜æ£€æµ‹
        grad_x = cv2.Sobel(depth, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(depth, cv2.CV_64F, 0, 1, ksize=3)
        
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–
        gradient_magnitude = (gradient_magnitude - gradient_magnitude.min()) / \
                           (gradient_magnitude.max() - gradient_magnitude.min() + 1e-8)
        
        edges = gradient_magnitude > threshold
        
        return edges
    
    def inpaint_depth(
        self,
        depth: np.ndarray,
        mask: np.ndarray,
        method: str = "telea"
    ) -> np.ndarray:
        """
        æ·±åº¦ä¿®å¤ - NEW
        
        Args:
            depth: æ·±åº¦å›¾
            mask: éœ€è¦ä¿®å¤çš„åŒºåŸŸ (True=éœ€è¦ä¿®å¤)
            method: ä¿®å¤æ–¹æ³• ['telea', 'ns']
            
        Returns:
            inpainted_depth: ä¿®å¤åçš„æ·±åº¦
        """
        # å½’ä¸€åŒ–åˆ°0-255
        depth_norm = ((depth - depth.min()) / (depth.max() - depth.min() + 1e-8) * 255).astype(np.uint8)
        
        # OpenCVä¿®å¤
        if method == "telea":
            inpainted = cv2.inpaint(depth_norm, mask.astype(np.uint8), 3, cv2.INPAINT_TELEA)
        else:
            inpainted = cv2.inpaint(depth_norm, mask.astype(np.uint8), 3, cv2.INPAINT_NS)
        
        # åå½’ä¸€åŒ–
        inpainted_depth = inpainted.astype(np.float32) / 255.0 * \
                         (depth.max() - depth.min()) + depth.min()
        
        return inpainted_depth
    
    def _preprocess(self, image: np.ndarray) -> torch.Tensor:
        """é¢„å¤„ç†"""
        # å½’ä¸€åŒ–åˆ°[0, 1]
        image = image.astype(np.float32) / 255.0
        
        # HWC -> CHW
        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
        
        # ImageNetæ ‡å‡†åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        image = (image - mean) / std
        
        return image
    
    def _normalize_depth(self, depth: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–æ·±åº¦åˆ°[0.5, max_depth]ç±³"""
        # ç§»é™¤æç«¯å€¼
        p1, p99 = np.percentile(depth, [1, 99])
        depth = np.clip(depth, p1, p99)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]
        depth = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
        
        # æ˜ å°„åˆ°[0.5, max_depth]ç±³
        depth = depth * (self.max_depth - 0.5) + 0.5
        
        return depth


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== Testing Depth Anything V2 ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    estimator = DepthAnythingV2(
        model_size="vitl",
        metric_depth=True,
        device=device
    )
    
    # æµ‹è¯•å›¾åƒ
    test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    print("\n1. åŸºç¡€æ·±åº¦ä¼°è®¡:")
    import time
    start = time.time()
    depth = estimator.estimate(test_image)
    elapsed = time.time() - start
    print(f"  Depth estimated in {elapsed:.3f}s")
    print(f"  Depth range: [{depth.min():.2f}, {depth.max():.2f}] meters")
    
    print("\n2. æ·±åº¦+ç½®ä¿¡åº¦:")
    depth, confidence = estimator.estimate_with_confidence(test_image)
    print(f"  Depth: {depth.shape}")
    print(f"  Confidence: {confidence.shape}, range: [{confidence.min():.2f}, {confidence.max():.2f}]")
    
    print("\n3. å¤šå°ºåº¦ä¼°è®¡:")
    depth_ms = estimator.estimate_multi_scale(test_image)
    print(f"  Multi-scale depth: {depth_ms.shape}")
    
    print("\n4. æ·±åº¦è¾¹ç¼˜:")
    edges = estimator.compute_depth_edges(depth)
    print(f"  Depth edges: {edges.shape}, {edges.sum()} edge pixels")
    
    print("\n5. æ·±åº¦ä¿®å¤:")
    mask = np.random.rand(*depth.shape) > 0.9
    inpainted = estimator.inpaint_depth(depth, mask)
    print(f"  Inpainted depth: {inpainted.shape}")
    
    print("\nâœ“ All tests passed!")