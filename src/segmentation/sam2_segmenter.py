"""
SAM 2 Segmenter - UPGRADED
æ”¯æŒè§†é¢‘åˆ†å‰²å’Œæ›´å¼ºçš„è¿½è¸ªèƒ½åŠ›
"""

import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
import cv2


class SAM2Segmenter:
    """SAM 2 åˆ†å‰²å™¨ - æ”¯æŒå›¾åƒå’Œè§†é¢‘"""
    
    def __init__(
        self,
        model_size: str = "large",  # tiny/small/base/large
        checkpoint: str = "data/checkpoints/sam2_hiera_large.pt",
        device: str = "cuda",
        mode: str = "image"  # image/video
    ):
        self.device = device
        self.model_size = model_size
        self.mode = mode
        
        print(f"ğŸ”„ Loading SAM 2 ({mode} mode): {model_size}")
        
        try:
            from sam2.build_sam import build_sam2
            from sam2.sam2_image_predictor import SAM2ImagePredictor
            from sam2.sam2_video_predictor import SAM2VideoPredictor
            
            # æ¨¡å‹é…ç½®æ˜ å°„
            model_cfg_map = {
                "tiny": "sam2_hiera_t.yaml",
                "small": "sam2_hiera_s.yaml",
                "base": "sam2_hiera_b.yaml",
                "large": "sam2_hiera_l.yaml"
            }
            
            model_cfg = model_cfg_map.get(model_size, "sam2_hiera_l.yaml")
            
            # æ„å»ºSAM2æ¨¡å‹
            sam2_model = build_sam2(model_cfg, checkpoint, device=device)
            
            # åˆ›å»ºé¢„æµ‹å™¨
            if mode == "image":
                self.predictor = SAM2ImagePredictor(sam2_model)
            else:  # video
                self.predictor = SAM2VideoPredictor(sam2_model)
            
            print(f"âœ“ SAM 2 loaded: {model_size} ({mode} mode)")
            
        except Exception as e:
            print(f"âŒ Failed to load SAM 2: {e}")
            print("  Please install: pip install git+https://github.com/facebookresearch/segment-anything-2.git")
            self.predictor = None
    
    # ============ å›¾åƒåˆ†å‰² ============
    
    @torch.no_grad()
    def segment_automatic(
        self,
        image: np.ndarray,
        points_per_side: int = 32,
        pred_iou_thresh: float = 0.88,
        stability_score_thresh: float = 0.95,
        min_mask_region_area: int = 100
    ) -> List[Dict]:
        """
        è‡ªåŠ¨åˆ†å‰² - SAM 2å¢å¼ºç‰ˆ
        
        Args:
            image: RGBå›¾åƒ (H, W, 3)
            points_per_side: æ¯è¾¹é‡‡æ ·ç‚¹æ•°
            pred_iou_thresh: IoUé˜ˆå€¼
            stability_score_thresh: ç¨³å®šæ€§é˜ˆå€¼
            min_mask_region_area: æœ€å°åŒºåŸŸé¢ç§¯
            
        Returns:
            masks: maskåˆ—è¡¨
        """
        if self.predictor is None:
            return self._dummy_masks(image.shape[:2])
        
        # SAM 2ä½¿ç”¨è‡ªåŠ¨maskç”Ÿæˆå™¨
        from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
        
        mask_generator = SAM2AutomaticMaskGenerator(
            model=self.predictor.model,
            points_per_side=points_per_side,
            pred_iou_thresh=pred_iou_thresh,
            stability_score_thresh=stability_score_thresh,
            min_mask_region_area=min_mask_region_area,
            crop_n_layers=1,
            crop_n_points_downscale_factor=2
        )
        
        masks = mask_generator.generate(image)
        
        # æŒ‰é¢ç§¯æ’åº
        masks = sorted(masks, key=lambda x: x['area'], reverse=True)
        
        return masks
    
    @torch.no_grad()
    def segment_with_prompts(
        self,
        image: np.ndarray,
        point_coords: Optional[np.ndarray] = None,
        point_labels: Optional[np.ndarray] = None,
        box: Optional[np.ndarray] = None,
        multimask_output: bool = True
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        æç¤ºå¼åˆ†å‰² - SAM 2
        
        Args:
            image: RGBå›¾åƒ
            point_coords: ç‚¹åæ ‡ (N, 2) [x, y]
            point_labels: ç‚¹æ ‡ç­¾ (N,) [1=å‰æ™¯, 0=èƒŒæ™¯]
            box: è¾¹ç•Œæ¡† (4,) [x1, y1, x2, y2]
            multimask_output: æ˜¯å¦è¾“å‡ºå¤šä¸ªmask
            
        Returns:
            masks: (M, H, W)
            scores: (M,)
            logits: (M, 256, 256)
        """
        if self.predictor is None:
            H, W = image.shape[:2]
            return (
                np.zeros((1, H, W), dtype=bool),
                np.array([0.5]),
                np.zeros((1, 256, 256))
            )
        
        # è®¾ç½®å›¾åƒ
        self.predictor.set_image(image)
        
        # é¢„æµ‹
        masks, scores, logits = self.predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            box=box,
            multimask_output=multimask_output
        )
        
        return masks, scores, logits
    
    # ============ è§†é¢‘åˆ†å‰² ============
    
    def init_video_state(
        self,
        video_path: str,
        offload_video_to_cpu: bool = True,
        async_loading_frames: bool = True
    ):
        """
        åˆå§‹åŒ–è§†é¢‘çŠ¶æ€ - SAM 2 Video NEW
        
        Args:
            video_path: è§†é¢‘è·¯å¾„æˆ–å¸§ç›®å½•
            offload_video_to_cpu: å¸è½½åˆ°CPUèŠ‚çœæ˜¾å­˜
            async_loading_frames: å¼‚æ­¥åŠ è½½å¸§
        """
        if self.mode != "video":
            raise ValueError("Video mode not enabled. Initialize with mode='video'")
        
        if self.predictor is None:
            return None
        
        # åˆå§‹åŒ–è§†é¢‘
        inference_state = self.predictor.init_state(
            video_path=video_path,
            offload_video_to_cpu=offload_video_to_cpu,
            async_loading_frames=async_loading_frames
        )
        
        return inference_state
    
    def add_object_prompt(
        self,
        inference_state,
        frame_idx: int,
        obj_id: int,
        points: Optional[np.ndarray] = None,
        labels: Optional[np.ndarray] = None,
        box: Optional[np.ndarray] = None
    ):
        """
        æ·»åŠ ç‰©ä½“æç¤º - åœ¨æŒ‡å®šå¸§
        
        Args:
            inference_state: è§†é¢‘æ¨ç†çŠ¶æ€
            frame_idx: å¸§ç´¢å¼•
            obj_id: ç‰©ä½“ID
            points: ç‚¹åæ ‡ (N, 2)
            labels: ç‚¹æ ‡ç­¾ (N,)
            box: è¾¹ç•Œæ¡† (4,)
        """
        if self.predictor is None:
            return
        
        # æ·»åŠ æç¤º
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=frame_idx,
            obj_id=obj_id,
            points=points,
            labels=labels,
            box=box
        )
        
        return out_obj_ids, out_mask_logits
    
    def propagate_in_video(
        self,
        inference_state,
        start_frame_idx: Optional[int] = None,
        max_frame_num_to_track: Optional[int] = None,
        reverse: bool = False
    ) -> Dict[int, Dict[int, np.ndarray]]:
        """
        åœ¨è§†é¢‘ä¸­ä¼ æ’­mask - SAM 2æ ¸å¿ƒåŠŸèƒ½
        
        Args:
            inference_state: è§†é¢‘æ¨ç†çŠ¶æ€
            start_frame_idx: èµ·å§‹å¸§
            max_frame_num_to_track: æœ€å¤§è¿½è¸ªå¸§æ•°
            reverse: æ˜¯å¦åå‘ä¼ æ’­
            
        Returns:
            video_segments: {
                frame_idx: {
                    obj_id: mask (H, W)
                }
            }
        """
        if self.predictor is None:
            return {}
        
        video_segments = {}
        
        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(
            inference_state,
            start_frame_idx=start_frame_idx,
            max_frame_num_to_track=max_frame_num_to_track,
            reverse=reverse
        ):
            frame_masks = {}
            for obj_id, mask_logits in zip(out_obj_ids, out_mask_logits):
                mask = (mask_logits > 0.0).cpu().numpy()
                frame_masks[obj_id] = mask
            
            video_segments[out_frame_idx] = frame_masks
        
        return video_segments
    
    # ============ é«˜çº§åŠŸèƒ½ ============
    
    def refine_with_features(
        self,
        masks: List[Dict],
        feature_map: torch.Tensor,
        similarity_threshold: float = 0.7
    ) -> List[Dict]:
        """
        ä½¿ç”¨DINOv2ç‰¹å¾ä¼˜åŒ–maskè¾¹ç•Œ
        
        Args:
            masks: SAMè¾“å‡ºçš„maskåˆ—è¡¨
            feature_map: DINOv2ç‰¹å¾å›¾ (1, D, H, W)
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            refined_masks: ä¼˜åŒ–åçš„maskåˆ—è¡¨
        """
        if len(masks) == 0:
            return masks
        
        feature_map = torch.nn.functional.normalize(feature_map, dim=1)
        B, D, H, W = feature_map.shape
        
        refined_masks = []
        
        for mask_dict in masks:
            mask = mask_dict['segmentation']
            
            # è°ƒæ•´maskå¤§å°
            mask_resized = cv2.resize(
                mask.astype(np.uint8),
                (W, H),
                interpolation=cv2.INTER_NEAREST
            ).astype(bool)
            
            # æå–maskç‰¹å¾
            mask_tensor = torch.from_numpy(mask_resized).to(feature_map.device)
            masked_features = feature_map * mask_tensor.unsqueeze(0).unsqueeze(0)
            
            if mask_tensor.sum() > 0:
                # è®¡ç®—å¹³å‡ç‰¹å¾
                avg_feature = masked_features.sum(dim=(2, 3)) / mask_tensor.sum()
                avg_feature = torch.nn.functional.normalize(avg_feature, dim=1)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                feature_flat = feature_map.reshape(1, D, -1)
                similarity = torch.matmul(avg_feature, feature_flat)
                similarity = similarity.reshape(1, H, W)
                
                # ä¼˜åŒ–mask
                refined_mask_small = (similarity[0] > similarity_threshold).cpu().numpy()
                
                # è°ƒæ•´å›åŸå§‹å°ºå¯¸
                refined_mask = cv2.resize(
                    refined_mask_small.astype(np.uint8),
                    (mask.shape[1], mask.shape[0]),
                    interpolation=cv2.INTER_NEAREST
                ).astype(bool)
                
                # æ›´æ–°mask
                mask_dict_copy = mask_dict.copy()
                mask_dict_copy['segmentation'] = refined_mask
                mask_dict_copy['area'] = int(refined_mask.sum())
                mask_dict_copy['bbox'] = self._mask_to_bbox(refined_mask)
                refined_masks.append(mask_dict_copy)
            else:
                refined_masks.append(mask_dict)
        
        return refined_masks
    
    @staticmethod
    def _mask_to_bbox(mask: np.ndarray) -> List[int]:
        """maskè½¬bbox"""
        rows = np.any(mask, axis=1)
        cols = np.any(mask, axis=0)
        
        if not rows.any() or not cols.any():
            return [0, 0, 0, 0]
        
        rmin, rmax = np.where(rows)[0][[0, -1]]
        cmin, cmax = np.where(cols)[0][[0, -1]]
        
        return [int(cmin), int(rmin), int(cmax - cmin), int(rmax - rmin)]
    
    def _dummy_masks(self, image_shape):
        """Dummy masks for testing"""
        H, W = image_shape
        return [{
            'segmentation': np.zeros((H, W), dtype=bool),
            'area': 0,
            'bbox': [0, 0, 0, 0],
            'predicted_iou': 0.0,
            'stability_score': 0.0
        }]


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== Testing SAM 2 Segmenter ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•å›¾åƒæ¨¡å¼
    print("\n1. Image Mode:")
    segmenter_img = SAM2Segmenter(
        model_size="large",
        device=device,
        mode="image"
    )
    
    test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    
    # è‡ªåŠ¨åˆ†å‰²
    masks = segmenter_img.segment_automatic(test_image)
    print(f"  Detected {len(masks)} masks")
    
    # æç¤ºå¼åˆ†å‰²
    masks, scores, logits = segmenter_img.segment_with_prompts(
        test_image,
        point_coords=np.array([[256, 256]]),
        point_labels=np.array([1])
    )
    print(f"  Prompt segmentation: {masks.shape}, scores: {scores}")
    
    # æµ‹è¯•è§†é¢‘æ¨¡å¼
    print("\n2. Video Mode:")
    segmenter_vid = SAM2Segmenter(
        model_size="large",
        device=device,
        mode="video"
    )
    print("  Video mode initialized")
    
    print("\nâœ“ All tests passed!")